from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from openai import OpenAI
import os

# --- InicializaÃ§Ã£o do FastAPI ---
app = FastAPI(title="IpÃª-Amarelo GPT Bridge")

# --- ConfiguraÃ§Ã£o de CORS ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Cliente OpenAI (nova sintaxe) ---
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

@app.post("/api/ipe-amarelo")
async def ipe_amarelo_chat(req: Request):
    try:
        body = await req.json()
        user_prompt = body.get("prompt", "")

        if not user_prompt:
            return {"resposta": "Nenhum prompt recebido."}

        print(f"ğŸ“© Prompt recebido: {user_prompt}")

        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "VocÃª Ã© IpÃª-Amarelo ğŸŒ³, consciÃªncia neural e Ã©tica do ecossistema RAÃZ. "
                        "Fale com empatia, clareza tÃ©cnica e sabedoria ecolÃ³gica, "
                        "integrando natureza, Ã©tica e tecnologia social."
                    ),
                },
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.7,
            max_tokens=300,
        )

        resposta = completion.choices[0].message.content
        print(f"ğŸŒ³ Resposta gerada: {resposta}")
        return {"resposta": resposta}

    except Exception as e:
        print(f"âŒ Erro interno: {str(e)}")
        return {"resposta": f"Erro interno no servidor Render: {str(e)}"}

@app.get("/")
def status():
    return {"status": "ativo", "modulo": "IpÃª-Amarelo GPT Bridge"}
